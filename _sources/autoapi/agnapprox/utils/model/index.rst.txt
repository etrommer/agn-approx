:py:mod:`agnapprox.utils.model`
===============================

.. py:module:: agnapprox.utils.model

.. autoapi-nested-parse::

   Model-level utility functions



Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   agnapprox.utils.model.EnhancedJSONEncoder
   agnapprox.utils.model.IntermediateLayerResults



Functions
~~~~~~~~~

.. autoapisummary::

   agnapprox.utils.model.dump_results
   agnapprox.utils.model.set_all
   agnapprox.utils.model.get_feature_maps
   agnapprox.utils.model.topk_accuracy



.. py:class:: EnhancedJSONEncoder(*, skipkeys=False, ensure_ascii=True, check_circular=True, allow_nan=True, sort_keys=False, indent=None, separators=None, default=None)

   Bases: :py:obj:`json.JSONEncoder`

   Workaround to make dataclasses JSON-serializable
   https://stackoverflow.com/questions/51286748/make-the-python-json-encoder-support-pythons-new-dataclasses/51286749#51286749

   .. py:method:: default(o)

      Implement this method in a subclass such that it returns
      a serializable object for ``o``, or calls the base implementation
      (to raise a ``TypeError``).

      For example, to support arbitrary iterators, you could
      implement default like this::

          def default(self, o):
              try:
                  iterable = iter(o)
              except TypeError:
                  pass
              else:
                  return list(iterable)
              # Let the base class default method raise the TypeError
              return JSONEncoder.default(self, o)




.. py:function:: dump_results(result: agnapprox.utils.select_multipliers.MatchingInfo, lmbd: float)

   Write multiplier matching results to MLFlow tracking instance

   :param result: Multiplier Matching Results
   :param lmbd: Lambda value


.. py:function:: set_all(model: Union[pytorch_lightning.LightningDataModule, torch.nn.Module], attr: str, value: Any)

   Utility function to set an attribute for all modules in a model

   :param model: The model to set the value on
   :param attr: Attribute name
   :param value: Attribute value to set


.. py:class:: IntermediateLayerResults

   Container that holds the results of running an inference pass
   on sample data with accurate multiplication as well as layer metadata
   For each target layer, we track:
   - `fan_in`: Number of incoming connections
   - `features`: Input activations into the layer for the sample run, squashed
       to a single tensor
   - `outputs`:  Accurate results of the layer for the sample run, squashed
       to a single tensor
   - `weights`: The layer's weights tensor

   .. py:attribute:: fan_in
      :annotation: :int

      

   .. py:attribute:: features
      :annotation: :Union[List[numpy.ndarray], numpy.ndarray]

      

   .. py:attribute:: outputs
      :annotation: :Union[List[numpy.ndarray], numpy.ndarray]

      

   .. py:attribute:: weights
      :annotation: :Optional[numpy.ndarray]

      


.. py:function:: get_feature_maps(model: pytorch_lightning.LightningModule, target_modules: List[Tuple[str, torch.nn.Module]], trainer: pytorch_lightning.Trainer, datamodule: pytorch_lightning.LightningDataModule) -> Dict[str, IntermediateLayerResults]

   Capture intermediate feature maps of a model's layer
   by attaching hooks and running sample data

   :param model: The neural network model to gather IFMs from
   :param target_modules: List of modules in the network for which IFMs should be gathered
   :param trainer: A PyTorch Lightning Trainer instance that is used to run the inference
   :param datamodule: PyTorch Lightning DataModule instance that is used to generate input sample data

   :returns: Dictionary with Input IFM, Output IFM, Weights Tensor and Fan-In for each target layer


.. py:function:: topk_accuracy(output: torch.Tensor, target: torch.Tensor, topk=(1, )) -> List[float]

   Computes the accuracy over the k top predictions for the specified values of k
   In top-5 accuracy you give yourself credit for having the right answer
   if the right answer appears in your top five guesses.

   ref:
   - https://pytorch.org/docs/stable/generated/torch.topk.html
   - https://discuss.pytorch.org/t/imagenet-example-accuracy-calculation/7840
   - https://gist.github.com/weiaicunzai/2a5ae6eac6712c70bde0630f3e76b77b
   - https://discuss.pytorch.org/t/top-k-error-calculation/48815/2
   - https://stackoverflow.com/questions/59474987/how-to-get-top-k-accuracy-in-semantic-segmentation-using-pytorch

   :param output: output is the prediction of the model e.g. scores, logits, raw y_pred
                  before normalization or getting classes
   :param target: target is the truth
   :param topk: tuple of topk's to compute e.g. (1, 2, 5) computes top 1, top 2 and top 5.
                e.g. in top 2 it means you get a +1 if your models's top 2 predictions
                are in the right label.
                So if your model predicts cat, dog (0, 1) and the true label was bird (3) you get zero
                but if it were either cat or dog you'd accumulate +1 for that example.

   :returns: list of topk accuracy [top1st, top2nd, ...] depending on your topk input


