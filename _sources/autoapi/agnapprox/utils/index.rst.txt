:py:mod:`agnapprox.utils`
=========================

.. py:module:: agnapprox.utils

.. autoapi-nested-parse::

   Approximate model helper functions



Submodules
----------
.. toctree::
   :titlesonly:
   :maxdepth: 1

   error_stats/index.rst
   model/index.rst
   select_multipliers/index.rst


Package Contents
----------------

Classes
~~~~~~~

.. autoapisummary::

   agnapprox.utils.EnhancedJSONEncoder
   agnapprox.utils.IntermediateLayerResults
   agnapprox.utils.LayerInfo
   agnapprox.utils.MatchingInfo



Functions
~~~~~~~~~

.. autoapisummary::

   agnapprox.utils.single_dist_mc
   agnapprox.utils.error_prediction
   agnapprox.utils.get_sample_population
   agnapprox.utils.population_prediction
   agnapprox.utils.to_distribution
   agnapprox.utils.error_calculation
   agnapprox.utils.dump_results
   agnapprox.utils.set_all
   agnapprox.utils.get_feature_maps
   agnapprox.utils.topk_accuracy
   agnapprox.utils.get_feature_maps
   agnapprox.utils.select_layer_multiplier
   agnapprox.utils.deploy_multipliers
   agnapprox.utils.select_multipliers



Attributes
~~~~~~~~~~

.. autoapisummary::

   agnapprox.utils.logger


.. py:function:: single_dist_mc(emap: numpy.ndarray, x_dist: numpy.ndarray, w_dist: numpy.ndarray, fan_in: float, num_samples: int = int(100000.0)) -> Tuple[float, float]

   Generate error mean and standard deviation using Monte Carlo
   approach as described in: https://arxiv.org/abs/1912.00700

   :param emap: The multiplier's error map
   :param x_dist: Operand distribution of activations
   :param w_dist: Operand distribution of weights
   :param fan_in: Incoming connections for layer
   :param num_samples: Number of Monte Carlo simulation runs. Defaults to int(1e5).

   :returns: Mean and standard deviation for a single operation


.. py:function:: error_prediction(emap: numpy.ndarray, x_dist: numpy.ndarray, w_dist: numpy.ndarray, fan_in: float) -> Tuple[float, float]

   Generate error mean and standard deviation using the
   global distribution of activations and weights

   :param emap: The multiplier's error map
   :param x_dist: Operand distribution of activations
   :param w_dist: Operand distribution of weights
   :param fan_in: Incoming connections for layer

   :returns: Mean and standard deviation for a single operation


.. py:function:: get_sample_population(tensor: numpy.ndarray, num_samples: int = 512) -> numpy.ndarray

   Randomly select samples from a tensor that cover the receptive field of one neuron

   :param tensor: Tensor to draw samples from
   :param num_samples: Number of samples to draw. Defaults to 512.

   :returns: Sampled 2D Tensor of shape [num_samples, tensor.shape[-1]]


.. py:function:: population_prediction(emap: numpy.ndarray, x_multidist: numpy.ndarray, w_dist: numpy.ndarray, fan_in: float) -> Tuple[float, float]

   Generate prediction of mean and standard deviation using several
   sampled local distributions

   :param emap: The multiplier's error map
   :param x_multidist: Array of several operand distributions for activations
   :param w_dist: Operand distribution of weights
   :param fan_in: Incoming connections for layer

   :returns: Mean and standard deviation for a single operation


.. py:function:: to_distribution(tensor: Optional[numpy.ndarray], min_val: int, max_val: int) -> Tuple[numpy.ndarray, numpy.ndarray]

   Turn tensor of weights/activations into a frequency distribution (i.e. build a histogram)

   :param tensor: Tensor to build histogram from
   :param min_val: Lowest possible operand value in tensor
   :param max_val: Highest possible operand value in tensor

   :returns: Tuple of Arrays where first array contains the full numerical range between
             min_val and max_val inclusively and second array contains the relative frequency
             of each operand

   :raises ValueError: If run before features maps have been populated
   :raises by call to `utils.model.get_feature_maps`:


.. py:function:: error_calculation(accurate: numpy.ndarray, approximate: numpy.ndarray, fan_in: float) -> Tuple[float, float]

   Calculate mean and standard deviation of the observed error between
   accurate computation and approximate computation

   :param accurate: Accurate computation results
   :param approximate: Approximate computation results
   :param fan_in: Number of incoming neuron connections

   :returns: Mean and standard deviation for a single operation


.. py:class:: EnhancedJSONEncoder(*, skipkeys=False, ensure_ascii=True, check_circular=True, allow_nan=True, sort_keys=False, indent=None, separators=None, default=None)

   Bases: :py:obj:`json.JSONEncoder`

   Workaround to make dataclasses JSON-serializable
   https://stackoverflow.com/questions/51286748/make-the-python-json-encoder-support-pythons-new-dataclasses/51286749#51286749

   .. py:method:: default(o)

      Implement this method in a subclass such that it returns
      a serializable object for ``o``, or calls the base implementation
      (to raise a ``TypeError``).

      For example, to support arbitrary iterators, you could
      implement default like this::

          def default(self, o):
              try:
                  iterable = iter(o)
              except TypeError:
                  pass
              else:
                  return list(iterable)
              # Let the base class default method raise the TypeError
              return JSONEncoder.default(self, o)




.. py:function:: dump_results(result: agnapprox.utils.select_multipliers.MatchingInfo, lmbd: float)

   Write multiplier matching results to MLFlow tracking instance

   :param result: Multiplier Matching Results
   :param lmbd: Lambda value


.. py:function:: set_all(model: Union[pytorch_lightning.LightningDataModule, torch.nn.Module], attr: str, value: Any)

   Utility function to set an attribute for all modules in a model

   :param model: The model to set the value on
   :param attr: Attribute name
   :param value: Attribute value to set


.. py:class:: IntermediateLayerResults

   Container that holds the results of running an inference pass
   on sample data with accurate multiplication as well as layer metadata
   For each target layer, we track:
   - `fan_in`: Number of incoming connections
   - `features`: Input activations into the layer for the sample run, squashed
       to a single tensor
   - `outputs`:  Accurate results of the layer for the sample run, squashed
       to a single tensor
   - `weights`: The layer's weights tensor

   .. py:attribute:: fan_in
      :annotation: :int

      

   .. py:attribute:: features
      :annotation: :Union[List[numpy.ndarray], numpy.ndarray]

      

   .. py:attribute:: outputs
      :annotation: :Union[List[numpy.ndarray], numpy.ndarray]

      

   .. py:attribute:: weights
      :annotation: :Optional[numpy.ndarray]

      


.. py:function:: get_feature_maps(model: pytorch_lightning.LightningModule, target_modules: List[Tuple[str, torch.nn.Module]], trainer: pytorch_lightning.Trainer, datamodule: pytorch_lightning.LightningDataModule) -> Dict[str, IntermediateLayerResults]

   Capture intermediate feature maps of a model's layer
   by attaching hooks and running sample data

   :param model: The neural network model to gather IFMs from
   :param target_modules: List of modules in the network for which IFMs should be gathered
   :param trainer: A PyTorch Lightning Trainer instance that is used to run the inference
   :param datamodule: PyTorch Lightning DataModule instance that is used to generate input sample data

   :returns: Dictionary with Input IFM, Output IFM, Weights Tensor and Fan-In for each target layer


.. py:function:: topk_accuracy(output: torch.Tensor, target: torch.Tensor, topk=(1, )) -> List[float]

   Computes the accuracy over the k top predictions for the specified values of k
   In top-5 accuracy you give yourself credit for having the right answer
   if the right answer appears in your top five guesses.

   ref:
   - https://pytorch.org/docs/stable/generated/torch.topk.html
   - https://discuss.pytorch.org/t/imagenet-example-accuracy-calculation/7840
   - https://gist.github.com/weiaicunzai/2a5ae6eac6712c70bde0630f3e76b77b
   - https://discuss.pytorch.org/t/top-k-error-calculation/48815/2
   - https://stackoverflow.com/questions/59474987/how-to-get-top-k-accuracy-in-semantic-segmentation-using-pytorch

   :param output: output is the prediction of the model e.g. scores, logits, raw y_pred
                  before normalization or getting classes
   :param target: target is the truth
   :param topk: tuple of topk's to compute e.g. (1, 2, 5) computes top 1, top 2 and top 5.
                e.g. in top 2 it means you get a +1 if your models's top 2 predictions
                are in the right label.
                So if your model predicts cat, dog (0, 1) and the true label was bird (3) you get zero
                but if it were either cat or dog you'd accumulate +1 for that example.

   :returns: list of topk accuracy [top1st, top2nd, ...] depending on your topk input


.. py:function:: get_feature_maps(model: pytorch_lightning.LightningModule, target_modules: List[Tuple[str, torch.nn.Module]], trainer: pytorch_lightning.Trainer, datamodule: pytorch_lightning.LightningDataModule) -> Dict[str, IntermediateLayerResults]

   Capture intermediate feature maps of a model's layer
   by attaching hooks and running sample data

   :param model: The neural network model to gather IFMs from
   :param target_modules: List of modules in the network for which IFMs should be gathered
   :param trainer: A PyTorch Lightning Trainer instance that is used to run the inference
   :param datamodule: PyTorch Lightning DataModule instance that is used to generate input sample data

   :returns: Dictionary with Input IFM, Output IFM, Weights Tensor and Fan-In for each target layer


.. py:data:: logger
   

   

.. py:function:: select_layer_multiplier(intermediate_results: agnapprox.utils.model.IntermediateLayerResults, multipliers: List[evoapproxlib.ApproximateMultiplier], max_noise: float, num_samples: int = 512) -> Tuple[str, float]

   Select a matching approximate multiplier for a single layer

   :param layer_ref_data: Reference input/output data generated from
                          a model run with *accurate* multiplication. This is used to calibrate
                          the layer standard deviation for the error estimate and determine the
                          distribution of numerical values in weights and activations.
   :param multipliers: Approximate Multiplier Error Maps, Performance Metrics and name
   :param max_noise: Learned allowable noise parameter (sigma_l)
   :param num_samples: Number of samples to draw from features for multi-population prediction.
                       Defaults to 512.

   :returns: Dictionary with name and performance metric of selected multiplier


.. py:class:: LayerInfo

   Multiplier Matching result for a single layer

   .. py:attribute:: name
      :annotation: :str

      

   .. py:attribute:: multiplier_name
      :annotation: :str

      

   .. py:attribute:: multiplier_performance_metric
      :annotation: :float

      

   .. py:attribute:: opcount
      :annotation: :float

      

   .. py:method:: relative_opcount(total_opcount: float)

      Calculate the relative contribution of this layer to the network's total operations

      :param total_opcount: Number of operations in the entire networks

      :returns:

                - 0: layer contributes no operations to the network's opcount
                - 1: layer conttibutes all operations to the network's opcount
      :rtype: float between 0..1 where


   .. py:method:: relative_energy_consumption(metric_max: float)

      Relative energy consumption of selected approximate multiplier

      :param metric_max: Highest possible value for performance metric
                         (typically that of the respective accurate multiplier)

      :returns:

                - 0: selected multiplier consumes no energy
                - 1: selected multiplier consumes the maximum amount of energy
      :rtype: float between 0..1 where



.. py:class:: MatchingInfo

   Multiplier Matching result for the entire model

   .. py:attribute:: layers
      :annotation: :List[LayerInfo]

      

   .. py:attribute:: metric_max
      :annotation: :float

      

   .. py:attribute:: opcount
      :annotation: :float

      

   .. py:method:: relative_energy_consumption()
      :property:

      Relative Energy Consumption compared to network without approximation
      achieved by the current AM configuration

      :returns: sum relative energy consumption for each layer,
                weighted with the layer's contribution to overall operations



.. py:function:: deploy_multipliers(model: agnapprox.nets.ApproxNet, matching_result: MatchingInfo, library)

   Deploy selected approximate multipliers to network

   :param model: Model to deploy multipliers to
   :param matching_result: Results of multiplier matching
   :param library: Library to load Lookup tables from


.. py:function:: select_multipliers(model: agnapprox.nets.ApproxNet, datamodule: pytorch_lightning.LightningDataModule, multipliers: List[evoapproxlib.ApproximateMultiplier], trainer: pytorch_lightning.Trainer) -> MatchingInfo

   Select matching Approximate Multipliers for all layers in a model

   :param model: Approximate Model with learned layer robustness parameters
   :param datamodule: Data Module to use for sampling runs
   :param library: Approximate Multiplier Library provider
   :param trainer: PyTorch Lightning Trainer instance to use for sampling run
   :param signed: Whether to select signed or unsigned instances from Multiplier library provide.
                  Defaults to True.

   :returns: Dictionary of Assignment results


